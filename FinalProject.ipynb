{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6be46bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull data\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from IPython.display import display, Image\n",
    "import imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d19040",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_int = {'corner':0, 'longpass':1, 'ontarget':2, 'penalty':3, 'substitution':4, 'throw-in':5}\n",
    "int_to_label = {0:'corner', 1:'longpass', 2:'ontarget', 3:'penalty', 4:'substitution', 5:'throw-in'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f486801",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoDataset(Dataset):\n",
    "    def __init__(self, video_folder, num_frames=5, transform=None):\n",
    "        self.video_folder = video_folder\n",
    "        self.num_frames = num_frames\n",
    "        self.transform = transform\n",
    "\n",
    "        self.file_names = []\n",
    "        self.labels_dic = {}\n",
    "        self.video_files = []\n",
    "        self.labels = []\n",
    "        for label in os.listdir(video_folder):\n",
    "            self.video_files += [f'{label}/{f}' for f in os.listdir(f'{video_folder}/{label}')]\n",
    "            self.labels += [label_to_int[label]]*len(os.listdir(f'{video_folder}/{label}'))\n",
    "            self.file_names += [f'{video_folder}/{label}/{f}' for f in os.listdir(f'{video_folder}/{label}')]\n",
    "            for f in os.listdir(f'{video_folder}/{label}'):\n",
    "                self.labels_dic[f'{label}/{f}'] = label_to_int[label]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_path = os.path.join(self.video_folder, self.video_files[idx])\n",
    "        frames = self.read_frames(video_path)\n",
    "\n",
    "        # Select 5 frames evenly spaced throughout the video\n",
    "        selected_frames = frames[::len(frames)//self.num_frames][:self.num_frames]\n",
    "\n",
    "        # Apply transformations to each frame if specified\n",
    "        if self.transform:\n",
    "            selected_frames = [self.transform(frame) for frame in selected_frames]\n",
    "\n",
    "        # Stack frames along a new dimension to create a single tensor\n",
    "        video_tensor = torch.stack(selected_frames, dim=0)\n",
    "\n",
    "        label = self.labels_dic[self.video_files[idx]]\n",
    "        return video_tensor, label\n",
    "\n",
    "\n",
    "    def read_frames(self, video_path):\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        frames = []\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frames.append(frame)\n",
    "        cap.release()\n",
    "        return frames\n",
    "\n",
    "# Example usage\n",
    "video_folder = './SoccerAct10Dataset'\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    #transforms.Resize((224, 224)),\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "dataset = VideoDataset(video_folder, num_frames=5, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6597a400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained VGG16 model\n",
    "vgg16 = models.vgg16(pretrained=True)\n",
    "\n",
    "# Remove the last fully connected layer of VGG16\n",
    "vgg16 = nn.Sequential(*list(vgg16.children())[:-1])\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "vgg16.eval()\n",
    "\n",
    "# Define your own model\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, vgg_features, num_classes, num_time_steps):\n",
    "        super(CustomModel, self).__init__()\n",
    "        self.num_time_steps = num_time_steps\n",
    "        self.vgg_features = vgg_features\n",
    "\n",
    "        # Assuming the VGG output size is 25088, adjust if needed\n",
    "        vgg_output_size = self._calculate_vgg_output_size()\n",
    "        self.reduce_vgg = nn.Linear(vgg_output_size,256)\n",
    "        self.time_distributed_fc = nn.ModuleList([\n",
    "            nn.Linear(256, 256) for _ in range(num_time_steps)\n",
    "        ])\n",
    "        \n",
    "        self.flat = nn.Flatten()\n",
    "        self.gru = nn.GRU(256*num_time_steps, 20, 2)  # Adjust input_size based on your needs\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.fc = nn.Linear(20, num_classes)\n",
    "    \n",
    "    def _calculate_vgg_output_size(self):\n",
    "        # Create a sample input tensor\n",
    "        sample_input = torch.randn(dataset[0][0].shape)\n",
    "\n",
    "        # Pass the sample input through the VGG model\n",
    "        vgg_output = self.vgg_features(sample_input)\n",
    "\n",
    "        # Calculate the total number of elements in the output tensor\n",
    "        vgg_output_size = vgg_output.view(vgg_output.size(0), -1).size(1)\n",
    "\n",
    "        return vgg_output_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        #with torch.autograd.detect_anomaly():\n",
    "        batch_size, num_frames, channels, height, width = x.size()\n",
    "        x = x.view(-1, channels, height, width)\n",
    "        x = self.vgg_features(x)\n",
    "        x = x.view(batch_size, num_frames, -1)\n",
    "        updated_x = torch.zeros_like(torch.empty(batch_size, self.num_time_steps, 256))\n",
    "        # Time-distributed stacked dense layers\n",
    "        for i in range(self.num_time_steps):\n",
    "            # Flatten the input before passing it to the dense layer\n",
    "            updated_x[:, i, :] = F.relu(self.time_distributed_fc[i](self.reduce_vgg(self.flat(x[:, i, :]))))\n",
    "\n",
    "        x = updated_x\n",
    "        x = self.flat(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        # Apply GRU\n",
    "        x, hidden = self.gru(x)\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        # Fully connected layer\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# Create an instance of your custom model\n",
    "num_classes = 6  # Change this to the number of classes in your task\n",
    "num_time_steps = 5\n",
    "custom_model = CustomModel(vgg16, num_classes, num_time_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553b3dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define cross-validation settings\n",
    "num_folds = 5  # Adjust as needed\n",
    "skf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# Define training parameters\n",
    "num_epochs = 5\n",
    "learning_rate = 0.001\n",
    "\n",
    "#torch.autograd.set_detect_anomaly(True)\n",
    "# Perform cross-validation\n",
    "for fold, (train_index, val_index) in enumerate(skf.split(range(len(dataset)), dataset.labels)):\n",
    "    print(f'Fold {fold + 1}/{num_folds}')\n",
    "\n",
    "    # Split data into training and validation sets for this fold\n",
    "    train_dataset = torch.utils.data.Subset(dataset, train_index)\n",
    "    val_dataset = torch.utils.data.Subset(dataset, val_index)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    # Create an instance of your custom model\n",
    "    custom_model = CustomModel(models.vgg16(pretrained=True).features, num_classes, num_time_steps)\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(custom_model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        custom_model.train()  # Set the model to training mode\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for videos, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = custom_model(videos)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # Print training statistics\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {running_loss / len(train_loader)}')\n",
    "\n",
    "    # Validation loop\n",
    "    custom_model.eval()  # Set the model to evaluation mode\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for videos, labels in val_loader:\n",
    "            outputs = custom_model(videos)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    # Print validation statistics\n",
    "    print(f'Validation Loss: {val_loss / len(val_loader)}, Accuracy: {100 * correct / total}%')\n",
    "\n",
    "print('Cross-validation finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1f6c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(custom_model.state_dict(), 'model1.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872affe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create a gif from frames\n",
    "def create_gif(frames, gif_path):\n",
    "    imageio.mimsave(gif_path, frames, duration=0.1)\n",
    "\n",
    "def predict_on_video(video_path):\n",
    "    idx = dataset.file_names.index(video_path)\n",
    "\n",
    "    # Make predictions\n",
    "    actual_label = dataset[idx][1]\n",
    "    with torch.no_grad():\n",
    "        predictions = custom_model(dataset[idx][0].unsqueeze(0))\n",
    "\n",
    "    # Convert predictions to probabilities\n",
    "    probabilities = torch.nn.functional.softmax(predictions, dim=1)\n",
    "\n",
    "    # Convert tensor to a list of probabilities\n",
    "    probabilities_list = probabilities.numpy().tolist()\n",
    "\n",
    "    # Create a dictionary with class labels and corresponding probabilities\n",
    "    result = {int_to_label[i]: probability for i, probability in enumerate(probabilities_list[0])}\n",
    "\n",
    "    return result, int_to_label[actual_label]\n",
    "\n",
    "\n",
    "# Function to display videos with labels and predictions\n",
    "def display_video_with_predictions(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Convert frame to RGB (OpenCV uses BGR by default)\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frames.append(frame_rgb)\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    # Save frames as GIF\n",
    "    gif_path = 'temp.gif'\n",
    "    imageio.mimsave(gif_path, frames, duration=0.1)\n",
    "    \n",
    "    predicted_labels, actual_label = predict_on_video(video_path)\n",
    "\n",
    "    print(f'Actual Label: {actual_label}')\n",
    "\n",
    "\n",
    "    for j in predicted_labels:\n",
    "        print(f'Predicted Probability for {j}: {predicted_labels[j]}')\n",
    "\n",
    "    # Display GIF in Jupyter Notebook\n",
    "    display(Image(filename=gif_path,width=500))\n",
    "\n",
    "# Display videos with predictions\n",
    "for label in os.listdir(video_folder):\n",
    "    for file in os.listdir(f'{video_folder}/{label}')[:1]:\n",
    "        display_video_with_predictions(f'{video_folder}/{label}/{file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59fbf16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
