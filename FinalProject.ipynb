{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6be46bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from IPython.display import display, Image\n",
    "import imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d19040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionaries to convert labels to integers and vice versa\n",
    "label_to_int = {'corner':0, 'longpass':1, 'ontarget':2, 'penalty':3, 'substitution':4, 'throw-in':5, 'foul':6, \\\n",
    "                'goalkick':7,'shortpass':8,'freekick':9}\n",
    "int_to_label = {0:'corner', 1:'longpass', 2:'ontarget', 3:'penalty', 4:'substitution', 5:'throw-in', 6:'foul', \\\n",
    "                7:'goalkick',8:'shortpass',9:'freekick'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f486801",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoDataset(Dataset):\n",
    "    def __init__(self, video_folder, num_frames=5, transform=None):\n",
    "        self.video_folder = video_folder\n",
    "        self.num_frames = num_frames\n",
    "        self.transform = transform\n",
    "\n",
    "        self.file_names = []\n",
    "        self.labels_dic = {}\n",
    "        self.video_files = []\n",
    "        self.labels = []\n",
    "        for label in os.listdir(video_folder):\n",
    "            self.video_files += [f'{label}/{f}' for f in os.listdir(f'{video_folder}/{label}')]\n",
    "            self.labels += [label_to_int[label]]*len(os.listdir(f'{video_folder}/{label}'))\n",
    "            self.file_names += [f'{video_folder}/{label}/{f}' for f in os.listdir(f'{video_folder}/{label}')]\n",
    "            for f in os.listdir(f'{video_folder}/{label}'):\n",
    "                self.labels_dic[f'{label}/{f}'] = label_to_int[label]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_path = os.path.join(self.video_folder, self.video_files[idx])\n",
    "        frames = self.read_frames(video_path)\n",
    "\n",
    "        # Select 5 frames evenly spaced throughout the video\n",
    "        selected_frames = frames[::len(frames)//self.num_frames][:self.num_frames]\n",
    "\n",
    "        # Apply transformations to each frame if specified\n",
    "        if self.transform:\n",
    "            selected_frames = [self.transform(frame) for frame in selected_frames]\n",
    "\n",
    "        # Stack frames along a new dimension to create a single tensor\n",
    "        video_tensor = torch.stack(selected_frames, dim=0)\n",
    "\n",
    "        # Add the label from the dictionary\n",
    "        label = self.labels_dic[self.video_files[idx]]\n",
    "        return video_tensor, label\n",
    "\n",
    "\n",
    "    def read_frames(self, video_path):\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        frames = []\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frames.append(frame)\n",
    "        cap.release()\n",
    "        return frames\n",
    "\n",
    "\n",
    "# Change video folder to wherever the video is\n",
    "video_folder = '../../cs/cs152/shared/isaac_skandda_josh/SoccerAct10Dataset'\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Generate dataset of videos for training\n",
    "dataset = VideoDataset(video_folder, num_frames=5, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6597a400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained resnet50 model\n",
    "resnet50 = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)\n",
    "# Remove the last fully connected layer of resnet50\n",
    "resnet50 = nn.Sequential(*list(resnet50.children())[:-1])\n",
    "\n",
    "# Load the pre-trained VGG16 model\n",
    "vgg16 = models.vgg16(pretrained=True)\n",
    "\n",
    "# Remove the last fully connected layer of VGG16\n",
    "vgg16 = nn.Sequential(*list(vgg16.children())[:-1])\n",
    "\n",
    "\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, vgg_features, resnet_features, num_classes, num_time_steps, \\\n",
    "                 reduce_size = 256, gru_or_LSTM = \"GRU\",vgg_or_res = \"VGG\"):\n",
    "        super(CustomModel, self).__init__()\n",
    "        self.num_time_steps = num_time_steps\n",
    "        self.gru_or_LSTM = gru_or_LSTM\n",
    "        self.vgg_or_res = vgg_or_res\n",
    "        \n",
    "        self.vgg_features = vgg_features\n",
    "        self.resnet_features = resnet_features\n",
    "\n",
    "        vgg_output_size = self._calculate_vgg_output_size()\n",
    "        resnet_output_size = self._calculate_resnet_output_size()\n",
    "        output_size = vgg_output_size if vgg_or_res == \"VGG\" else resnet_output_size \n",
    "        \n",
    "        self.reduce_layer = nn.Linear(output_size,reduce_size)\n",
    "        self.time_distributed_fc = nn.ModuleList([\n",
    "            nn.Linear(reduce_size, reduce_size) for _ in range(num_time_steps)\n",
    "        ])\n",
    "        \n",
    "        self.flat = nn.Flatten()\n",
    "        self.gru = nn.GRU(reduce_size, 20, 2, batch_first = True)\n",
    "        self.lstm = nn.LSTM(reduce_size, 20 , 2, batch_first = True)\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "        self.fc = nn.Linear(20, num_classes)\n",
    "\n",
    "        for param in self.vgg_features.parameters():\n",
    "                param.requires_grad = False\n",
    "            \n",
    "        for param in self.resnet_features.parameters():\n",
    "                param.requires_grad = False\n",
    "    \n",
    "    def _calculate_vgg_output_size(self):\n",
    "        sample_input = torch.randn(dataset[0][0].shape)\n",
    "\n",
    "        # Pass the sample input through the VGG model\n",
    "        vgg_output = self.vgg_features(sample_input)\n",
    "\n",
    "        # Calculate the total number of elements in the output tensor\n",
    "        vgg_output_size = vgg_output.view(vgg_output.size(0), -1).size(1)\n",
    "\n",
    "        return vgg_output_size\n",
    "    \n",
    "    def _calculate_resnet_output_size(self):\n",
    "        sample_input = torch.randn(dataset[0][0].shape)\n",
    "\n",
    "        # Pass the sample input through the resnet model\n",
    "        resnet_output = self.resnet_features(sample_input)\n",
    "\n",
    "        # Calculate the total number of elements in the output tensor\n",
    "        resnet_output_size = resnet_output.view(resnet_output.size(0), -1).size(1)\n",
    "\n",
    "        return resnet_output_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, num_frames, channels, height, width = x.size()\n",
    "\n",
    "        # Resize input tensor so it can be passed to VGG/resnet\n",
    "        x = x.view(-1, channels, height, width)\n",
    "\n",
    "        # Pass resized input tensor through VGG/resnet\n",
    "        if self.vgg_or_res == \"VGG\":\n",
    "            x = self.vgg_features(x)\n",
    "        else:\n",
    "            x = self.resnet_features(x)\n",
    "\n",
    "        # Resize to have batch_size as first dimension again\n",
    "        x = x.view(batch_size, num_frames, -1)\n",
    "        updated_x = torch.zeros_like(torch.empty(batch_size, self.num_time_steps, 256))\n",
    "\n",
    "        # Apply time-distributed stacked dense layers\n",
    "        for i in range(self.num_time_steps):\n",
    "            # Reduce the input, using the reduce layer before passing it to the dense layer\n",
    "            updated_x[:, i, :] = F.relu(self.time_distributed_fc[i](F.relu(self.reduce_layer(x[:, i, :]))))\n",
    "\n",
    "        x = updated_x\n",
    "\n",
    "        # Apply GRU/LSTM\n",
    "        if self.gru_or_LSTM == \"GRU\":\n",
    "            _,x = self.gru(x)\n",
    "            x = x[1,:,:,]\n",
    "        else:\n",
    "            x,_ = self.lstm(x)\n",
    "            x = x[:,-1,:]\n",
    "\n",
    "        # Apply dropout layer\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Apply fully connected layer\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "num_classes = 10\n",
    "num_time_steps = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4d05d4-c937-4724-9db7-34d0b3325a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "train_ratio = 0.8\n",
    "test_ratio = 1 - train_ratio\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Calculate the number of samples for the train and test sets\n",
    "num_samples = len(dataset)\n",
    "num_train = int(train_ratio * num_samples)\n",
    "num_test = num_samples - num_train\n",
    "\n",
    "random.seed(42)\n",
    "# Generate random indices for the train and test sets\n",
    "random_indices = random.sample(range(num_samples), num_samples)\n",
    "train_indices = random_indices[:num_train]\n",
    "test_indices = random_indices[num_train:]\n",
    "\n",
    "# Separate dataset into train and test sets\n",
    "train_dataset = torch.utils.data.Subset(dataset, train_indices)\n",
    "test_dataset = torch.utils.data.Subset(dataset, test_indices)\n",
    "\n",
    "# Convert datasets into loaders so we can use for training or evaluation\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2a750f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model architecture\n",
    "custom_model = CustomModel(vgg16, resnet50, num_classes, num_time_steps, reduce_size=256, gru_or_LSTM = \"GRU\",vgg_or_res = \"VGG\")\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(custom_model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Loop over the dataset every epoch\n",
    "for epoch in range(5):  \n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in tqdm(enumerate(train_loader, 0),total = len(train_loader)):\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = custom_model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 10 == 9:    # print every 10 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 10:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1eea489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change as needed\n",
    "model_name = 'model1'\n",
    "torch.save(custom_model.state_dict(), f'{model_name}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872affe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create a gif from frames\n",
    "def create_gif(frames, gif_path):\n",
    "    imageio.mimsave(gif_path, frames, duration=0.1)\n",
    "\n",
    "# Function to return the predictions and actual label for a given video\n",
    "def predict_on_video(video_path):\n",
    "    idx = dataset.file_names.index(video_path)\n",
    "\n",
    "    # Make predictions\n",
    "    actual_label = dataset[idx][1]\n",
    "    with torch.no_grad():\n",
    "        predictions = custom_model(dataset[idx][0].unsqueeze(0))\n",
    "\n",
    "    # Convert predictions to probabilities\n",
    "    probabilities = torch.nn.functional.softmax(predictions, dim=1)\n",
    "\n",
    "    # Convert tensor to a list of probabilities\n",
    "    probabilities_list = probabilities.numpy().tolist()\n",
    "\n",
    "    # Create a dictionary with class labels and corresponding probabilities\n",
    "    result = {int_to_label[i]: probability for i, probability in enumerate(probabilities_list[0])}\n",
    "\n",
    "    return result, int_to_label[actual_label]\n",
    "\n",
    "\n",
    "# Function to display videos with labels and predictions\n",
    "def display_video_with_predictions(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Convert frame to RGB (OpenCV uses BGR by default)\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frames.append(frame_rgb)\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    # Save frames as GIF\n",
    "    gif_path = 'temp.gif'\n",
    "    imageio.mimsave(gif_path, frames, duration=0.1)\n",
    "    \n",
    "    predicted_labels, actual_label = predict_on_video(video_path)\n",
    "\n",
    "    print(f'Actual Label: {actual_label}')\n",
    "\n",
    "\n",
    "    for j in predicted_labels:\n",
    "        print(f'Predicted Probability for {j}: {predicted_labels[j]}')\n",
    "\n",
    "    display(Image(filename=gif_path,width=500))\n",
    "\n",
    "# Display sample videos with predictions to validate model\n",
    "for label in os.listdir(video_folder):\n",
    "    for file in os.listdir(f'{video_folder}/{label}')[:1]:\n",
    "        display_video_with_predictions(f'{video_folder}/{label}/{file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59fbf16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def calculate_classification_metrics(predictions, targets):\n",
    "    # Convert PyTorch tensors to numpy arrays\n",
    "    predictions = predictions.detach().numpy()\n",
    "    targets = targets.detach().numpy()\n",
    "\n",
    "    # Convert probabilities to class labels\n",
    "    predicted_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(targets, predicted_labels)\n",
    "\n",
    "    # Calculate precision, recall, and F1 score\n",
    "    precision = precision_score(targets, predicted_labels, average='weighted')\n",
    "    recall = recall_score(targets, predicted_labels, average='weighted')\n",
    "    f1 = f1_score(targets, predicted_labels, average='weighted')\n",
    "\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "\n",
    "model = CustomModel(vgg16, resnet50, num_classes, num_time_steps, gru_or_LSTM = \"GRU\",vgg_or_res = \"RES\")\n",
    "model.load_state_dict(torch.load(f'{model_name}.pth'), strict=False)\n",
    "model.eval()\n",
    "\n",
    "# Lists to store predictions and targets\n",
    "all_predictions = []\n",
    "all_targets = []\n",
    "\n",
    "# Iterate through the test data loader\n",
    "for inputs, targets in test_loader:\n",
    "    with torch.no_grad():\n",
    "        outputs = model(inputs)\n",
    "    all_predictions.append(outputs)\n",
    "    all_targets.append(targets)\n",
    "\n",
    "all_predictions = torch.cat(all_predictions)\n",
    "all_targets = torch.cat(all_targets)\n",
    "\n",
    "accuracy, precision, recall, f1 = calculate_classification_metrics(all_predictions, all_targets)\n",
    "print(f\"Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412ddac0-83a1-4db5-8b5d-f2a5d76701c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_folds = 5 \n",
    "skf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# Define training parameters\n",
    "num_epochs = 5\n",
    "learning_rate = 0.001\n",
    "\n",
    "\n",
    "# Perform cross-validation\n",
    "for fold, (train_index, val_index) in enumerate(skf.split(range(len(dataset)), dataset.labels)):\n",
    "    print(f'Fold {fold + 1}/{num_folds}')\n",
    "\n",
    "    # Split data into training and validation sets for this fold\n",
    "    train_dataset = torch.utils.data.Subset(dataset, train_index)\n",
    "    val_dataset = torch.utils.data.Subset(dataset, val_index)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    # Create an instance of your custom model\n",
    "    custom_model = CustomModel(models.vgg16(pretrained=True).features, num_classes, num_time_steps, \\\n",
    "                               reduce_size=256, gru_or_LSTM = \"GRU\",vgg_or_res = \"VGG\")\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(custom_model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        custom_model.train()  # Set the model to training mode\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for videos, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = custom_model(videos)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # Print training statistics\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {running_loss / len(train_loader)}')\n",
    "\n",
    "    # Validation loop\n",
    "    custom_model.eval()  # Set the model to evaluation mode\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for videos, labels in val_loader:\n",
    "            outputs = custom_model(videos)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    # Print validation statistics\n",
    "    print(f'Validation Loss: {val_loss / len(val_loader)}, Accuracy: {100 * correct / total}%')\n",
    "\n",
    "print('Cross-validation finished')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
